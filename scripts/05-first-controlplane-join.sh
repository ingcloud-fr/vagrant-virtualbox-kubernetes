#!/bin/bash
set -e

# ======================================
# 05 - N≈ìud multi/controlplane Kubernetes 
# ======================================

# Ce script initialise le cluster Kubernetes
# Il s'ex√©cute uniquement sur le controlplane

# Variables
K8S_VERSION="1.32"
CNI_PLUGIN=${CNI_PLUGIN:-cilium}
POD_CIDR="10.244.0.0/16"
CLUSTER_NAME=${CLUSTER_NAME:-k8s}
CONTROLPLANE_VIP="${CONTROLPLANE_VIP:-192.168.1.210}"
NUM_CONTROLPLANE="${NUM_CONTROLPLANE:-1}"
IP_START="${IP_START:-192.168.1.200}"
BASE_IP=$(echo "$IP_START" | cut -d. -f1-3)
START_OCTET=$(echo "$IP_START" | cut -d. -f4)

echo "‚öôÔ∏è  Installating the first controlplane ..."

# R√©cup√©ration de l'IP locale (export√©e dans 01)
MY_IP=$(grep PRIMARY_IP /etc/environment | cut -d= -f2)

# G√©n√©ration du fichier /vagrant/hosts 
cat <<EOF > /vagrant/hosts
# Hosts list generated by Vagrant
127.0.0.1 localhost
# The following lines are desirable for IPv6 capable hosts
::1	ip6-localhost	ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
ff02::3	ip6-allhosts
EOF

# # Add haproxy-vip in /vagrant/hosts
# if [ "$NUM_CONTROLPLANE" -gt 1 ]; then
#   echo "$CONTROLPLANE_VIP $CLUSTER_NAME-haproxy-vip" >> /vagrant/hosts
# fi

# Ajout de l'IP locale
echo "$MY_IP $(hostname)" >> /vagrant/hosts

# If there are several controlplanes, the API endpoint is the VIP else it MY_IP
if [ "$NUM_CONTROLPLANE" -gt 1 ]; then
  ADVERTISE_IP=$CONTROLPLANE_VIP
else
  ADVERTISE_IP=$MY_IP
fi

# INIT
echo "‚öôÔ∏è  Pull images for kudeadm ..."
kubeadm config images pull
echo "‚öôÔ∏è  Cluster initialization with kubeadm init --control-plane-endpoint $ADVERTISE_IP:6443 ... "
kubeadm init --control-plane-endpoint $ADVERTISE_IP:6443 \
  --upload-certs \
  --apiserver-advertise-address $MY_IP \
  --pod-network-cidr=$POD_CIDR

# R√©cup√©ration du hash et token join
JOIN_COMMAND_CONTROLPLANE=$(kubeadm token create --print-join-command)
HASH=$(echo "$JOIN_COMMAND_CONTROLPLANE" | grep -o 'sha256:[a-f0-9]*')
TOKEN=$(echo "$JOIN_COMMAND_CONTROLPLANE" | awk '{print $5}')

# R√©cup√©ration de la cl√© pour le join en mode control-plane
CERT_KEY=$(kubeadm init phase upload-certs --upload-certs | tail -1)

if [ "$NUM_CONTROLPLANE" -gt 1 ]; then
  # G√©n√©ration du script de join pour les autres controlplanes
  echo "üõ†Ô∏è  Creating join script for secondary controlplanes ..." 
  JOIN_CONTROLPLANE="kubeadm join ${CONTROLPLANE_VIP}:6443 --control-plane --token ${TOKEN} --discovery-token-ca-cert-hash ${HASH} --certificate-key ${CERT_KEY}"
  echo "$JOIN_CONTROLPLANE" > /vagrant/join-controlplane-${CLUSTER_NAME}.sh
  cat <<EOF > /vagrant/join-controlplane-${CLUSTER_NAME}.sh
#!/bin/bash
$JOIN_CONTROLPLANE "\$@"
EOF
  chmod +x /vagrant/join-controlplane-${CLUSTER_NAME}.sh
  chown vagrant:vagrant /vagrant/join-controlplane-${CLUSTER_NAME}.sh
fi
# NOTE : the ending "\$@" allow secondary controlplan to pass argument when invoking the script (as --apiserver-advertise-address=xxx)

# G√©n√©ration du script de join pour les workers
echo "üõ†Ô∏è  Creating join script for workers ..."  
JOIN_WORKER="kubeadm join ${ADVERTISE_IP}:6443 --token ${TOKEN} --discovery-token-ca-cert-hash ${HASH}"
echo "$JOIN_WORKER" > /vagrant/join-${CLUSTER_NAME}.sh
cat <<EOF > /vagrant/join-${CLUSTER_NAME}.sh
#!/bin/bash
$JOIN_WORKER "\$@"
EOF
chmod +x /vagrant/join-${CLUSTER_NAME}.sh
chown vagrant:vagrant /vagrant/join-${CLUSTER_NAME}.sh

# Configuration de kubectl pour le user vagrant
mkdir -p /home/vagrant/.kube
cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
chmod 600 /home/vagrant/.kube/config
chown -R vagrant:vagrant /home/vagrant/.kube

# Copie du kubeconfig pour les autres nodes
cp /etc/kubernetes/admin.conf /vagrant/admin.conf
chown vagrant:vagrant /vagrant/admin.conf

# Attente (utile encore ??)
# echo "‚è≥ Attente que l'API Kubernetes soit accessible via la VIP ($CONTROLPLANE_VIP:6443)..."
# start=$(date +%s)
# timeout=120
# until curl -k https://$CONTROLPLANE_VIP:6443/version >/dev/null 2>&1; do
#   echo -n "..."
#   sleep 3
#   now=$(date +%s)
#   if (( now - start > timeout )); then
#     echo ""
#     echo "üö® Timeout atteint. L'API Kubernetes n'est pas accessible via la VIP"
#     exit 1
#   fi
# done

# echo ""
# echo "‚úÖ API Kubernetes accessible via la VIP. Suite du provisioning..."

for i in {1..60}; do
  su - vagrant -c "kubectl get nodes &>/dev/null" && break
  echo "‚è≥ Attente que l'API Kubernetes soit disponible pour installation du CNI..."
  sleep 2
done

# Installation CRDs API Gateway
VERSION=$(curl -s https://api.github.com/repos/kubernetes-sigs/gateway-api/releases/latest | grep '"tag_name":' | cut -d '"' -f4)
echo "üì¶  Installing LAST Gateway API CRDs : $VERSION"
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/$VERSION/standard-install.yaml

# Installation du CNI
HOSTNAME=$(hostname)
# echo "üêû DEBUG HOSTNAME = $HOSTNAME"
echo "üîß CNI installation : $CNI_PLUGIN"
if [[ "$CNI_PLUGIN" == "flannel" ]]; then
  su - vagrant -c "kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml"
elif [[ "$CNI_PLUGIN" == *"cilium"* ]]; then # $CNI_PLUGIN contains "cilium"
  if ! command -v helm &> /dev/null; then
    echo "üì¶ Installation Helm v3..."
    curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  fi
  echo "üîß Adding and update helm repo Cilium..."
  su - vagrant -c "helm repo add cilium https://helm.cilium.io/"
  su - vagrant -c "helm repo update"
  # Base cilium
  echo "üõ†Ô∏è  Cilium base installation..."
  su - vagrant -c "helm install cilium cilium/cilium \
    --namespace kube-system \
    --set k8sServiceHost=$HOSTNAME \
    --set k8sServicePort=6443 \
    --set operator.replicas=2 \
    --timeout 10m0s \
    --wait \
    --wait-for-jobs"

  if [[ "$CNI_PLUGIN" == *"l7"* ]]; then
    su - vagrant -c 'helm list -n kube-system'
    echo "‚öôÔ∏è Cilium upgrade with encryption"
    su - vagrant -c "helm upgrade cilium cilium/cilium \
      --namespace kube-system \
      --reuse-values \
      --set enableL7Proxy=true"
  fi

  if [[ "$CNI_PLUGIN" == *"encryption"* ]]; then
    su - vagrant -c 'helm list -n kube-system'
    echo "üîß Cilium upgrade with Wireguard encryption"
    su - vagrant -c "helm upgrade cilium cilium/cilium \
      --namespace kube-system \
      --reuse-values \
      --set kubeProxyReplacement=true \
      --set kubeProxyReplacementStrict=true \
      --set encryption.enabled=true \
      --set encryption.type=wireguard"
  fi

  if [[ "$CNI_PLUGIN" == *"gwapi"* ]]; then
    su - vagrant -c 'helm list -n kube-system'
    echo "‚öôÔ∏è Cilium upgrade with Gateway API"
    su - vagrant -c "helm upgrade cilium cilium/cilium \
      --namespace kube-system \
      --reuse-values \
      --set kubeProxyReplacement=true \
      --set gatewayAPI.enabled=true"
  fi


  if [[ "$CNI_PLUGIN" == *"mtls"* ]]; then
    su - vagrant -c 'helm list -n kube-system'
    echo "üîß Cilium upgrade with mTLS Spiffe"
    su - vagrant -c "helm upgrade cilium cilium/cilium \
      --namespace kube-system \
      --reuse-values \
      --set authentication.mutual.spire.enabled=true \
      --set authentication.mutual.spire.install.enabled=true"

    # NOTE : 
    # About : --set authentication.mutual.spire.install.server.dataStorage.enabled=false \
    # The spire server default installation requires PersistentVolumeClaim support in the cluster.
    #¬†For lab or local cluster, you can switch to in-memory storage by passing authentication.mutual.spire.install.server.dataStorage.enabled=false 
    #¬†to the installation command, at the cost of re-creating all data when the SPIRE server pod is restarted.
    #
    # Or we can create a PV (1 Gi min)for the spire server
    echo "üõ¢Ô∏è  Creating the PV spire-pv for the Spire Sever ..."
    sudo mkdir -p /mnt/data-spire-server
    su - vagrant -c "kubectl apply -f -" <<EOF
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: spire-pv
    spec:
      capacity:
        storage: 1Gi
      volumeMode: Filesystem
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      storageClassName: ""
      hostPath:
        path: /mnt/data-spire-server
EOF
  fi
  if [[ "$CNI_PLUGIN" == *"l2lb"* ]]; then
    echo "üõ†Ô∏è  Cilium upgrade with l2 announcements (loadbalancing)"
    su - vagrant -c "helm upgrade cilium cilium/cilium \
      --namespace kube-system \
      --reuse-values \
      --set kubeProxyReplacement=true \
      --set l2announcements.enabled=true \
      --set l2announcements.leaseDuration="3s" \
      --set l2announcements.leaseRenewDeadline="1s" \
      --set l2announcements.leaseRetryPeriod="500ms" \
      --set devices[0]=eth0 --set devices[1]=eth1 \
      --set externalIPs.enabled=true"
    # eth1 bridge et eth0 nat
  fi

  echo "‚öôÔ∏è  CLI cilium installation"
  CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
  CLI_ARCH=amd64
  if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
  curl -fsSL --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
  sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
  sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
  rm -f cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
  su - vagrant -c "cilium version"
  echo "üî® Hubble Cilium installation ..."
  HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
  HUBBLE_ARCH=amd64
  if [ "$(uname -m)" = "aarch64" ]; then HUBBLE_ARCH=arm64; fi
  curl -fsSL --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
  sha256sum --check hubble-linux-${HUBBLE_ARCH}.tar.gz.sha256sum
  sudo tar xzvfC hubble-linux-${HUBBLE_ARCH}.tar.gz /usr/local/bin
  rm hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
  su - vagrant -c "hubble version"

  # Enable Hubble
  su - vagrant -c "cilium hubble enable"
  echo "üí°  Hubble is enabled ! To use it forward port and launch hubble : $ cilium hubble port-forward & AND $ hubble observe"

else
  echo "üõë CNI '$CNI_PLUGIN' unkown."
  exit 1
fi

# Removing taint node-role.kubernetes.io/control-plane:NoSchedule
echo "üîß  Removing Taint node-role.kubernetes.io/control-plane:NoSchedule"
NODE_NAME=$(hostname)
su - vagrant -c "kubectl taint node \"$NODE_NAME\" node-role.kubernetes.io/control-plane-"

#############  POUR DEBUG ##############
# cat <<EOF > /etc/env-k8s-vars
# export CONTROLPLANE_VIP=CONTROLPLANE_VIP
# export MY_IP=$MY_IP
# export K8S_VERSION=K8S_VERSION
# export POD_CIDR=POD_CIDR
# export BASE_IP=BASE_IP
# export START_OCTET=START_OCTET
# export NUM_CONTROLPLANE=NUM_CONTROLPLANE
# export CONTAINER_RUNTIME=CONTAINER_RUNTIME
# export TOKEN=$TOKEN
# export HASH=$HASH
# export CERT_KEY=$CERT_KEY
# export JOIN_COMMAND_CONTROLPLANE=$JOIN_COMMAND_CONTROLPLANE
# export JOIN_COMMAND=$JOIN_COMMAND

# EOF
########################################

echo "üèÅ Primary CONTROLPLANE node $(hostname) successfully joined the Kubernetes cluster !"